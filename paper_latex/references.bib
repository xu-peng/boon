@book{adams1995hitchhiker,
  title={The Hitchhiker's Guide to the Galaxy},
  author={Adams, D.},
  isbn={9781417642595},
  url={http://books.google.com/books?id=W-xMPgAACAAJ},
  year={1995},
  publisher={San Val}
}


@article{Chelba2014,
abstract = {We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35{\%} reduction in perplexity, or 10{\%} reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.},
archivePrefix = {arXiv},
arxivId = {1312.3005},
author = {Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},
eprint = {1312.3005},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/41880.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Benchmark,Language modeling,Reproducible research},
mendeley-groups = {DeepLearning},
pages = {2635--2639},
title = {{One billion word benchmark for measuring progress in statistical language modeling}},
year = {2014}
}


@article{Jozefowicz2016,
abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.},
archivePrefix = {arXiv},
arxivId = {1602.02410},
author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
eprint = {1602.02410},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1602.02410.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
mendeley-groups = {DeepLearning},
title = {{Exploring the Limits of Language Modeling}},
url = {http://arxiv.org/abs/1602.02410},
year = {2016}
}


@inproceedings{hill2015goldilocks,
  title={The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations},
  author={Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
  booktitle={ICLR},
  year={2016}
}

@article{weston2014memory,
  title={Memory networks},
  author={Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  journal={arXiv preprint arXiv:1410.3916},
  year={2014}
}

@inproceedings{hermann2015teaching,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  booktitle={NIPS},
  year={2015}
}


@article{taylor1953cloze,
  title={Cloze procedure: a new tool for measuring readability.},
  author={Taylor, Wilson L},
  journal={Journalism and Mass Communication Quarterly},
  volume={30},
  number={4},
  pages={415},
  year={1953},
  publisher={Association for Education in Journalism, etc.}
}

@article{Cho2014,
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
eprint = {1406.1078},
journal = {Empirical Methods in Natural Language Processing (EMNLP)},
mendeley-groups = {DeepLearning},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078v3},
year = {2014}
}


@article{lowe2015ubuntu,
  title={The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems},
  author={Lowe, Ryan and Pow, Nissan and Serban, Iulian and Pineau, Joelle},
  journal={arXiv preprint arXiv:1506.08909},
  year={2015}
}

@article{shang2015neural,
  title={Neural Responding Machine for Short-Text Conversation},
  author={Shang, Lifeng and Lu, Zhengdong and Li, Hang},
  journal={arXiv preprint arXiv:1503.02364},
  year={2015}
}

@article{vinyals2015neural,
  title={A Neural Conversational Model},
  author={Vinyals, Oriol and Le, Quoc},
  journal={arXiv preprint arXiv:1506.05869},
  year={2015}
}

@inproceedings{lai2015recurrent,
  title={Recurrent convolutional neural networks for text classification},
  author={Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
  booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}

@article{kim2014convolutional,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv preprint arXiv:1408.5882},
  year={2014}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}


@MISC{Bastien-Theano-2012,
        author = {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
         title = {Theano: new features and speed improvements},
          year = {2012},
  howpublished = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop}
}

@article{VanMerrienboer2015,
abstract = {We introduce two Python frameworks to train neural networks on large datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support. It facilitates the training of complex neural network models by providing parametrized Theano operations, attaching metadata to Theano's symbolic computational graph, and providing an extensive set of utilities to assist training the networks, e.g. training algorithms, logging, monitoring, visualization, and serialization. Fuel provides a standard format for machine learning datasets. It allows the user to easily iterate over large datasets, performing many types of pre-processing on the fly.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.00619v1},
author = {van Merrienboer, Bart and Bahdanau, Dzmitry and Dumoulin, Vincent and Serdyuk, Dmitriy and Warde-farley, David and Chorowski, Jan and Bengio, Yoshua},
eprint = {arXiv:1506.00619v1},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1506.00619v1.pdf:pdf},
keywords = {gpgpu,large-scale machine learning,neural networks},
mendeley-groups = {DeepLearning},
pages = {1--5},
title = {{Blocks and Fuel : Frameworks for deep learning}},
url = {http://arxiv.org/abs/1506.00619},
year = {2015}
}


@article{Pennington2014a,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75\% accuracy, an improvement of 11\% over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/glove.pdf:pdf},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {DeepLearning},
pages = {1532--1543},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}

@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v5},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v5},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1412.6980.pdf:pdf},
journal = {International Conference on Learning Representations},
mendeley-groups = {DeepLearning},
pages = {1--13},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}

@article{mesnil2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.5335v7},
author = {Mesnil, Gr\'{e}goire and Mikolov, Tomas and Ranzato, Marc'Aurelio and Bengio, Yoshua},
eprint = {arXiv:1412.5335v7},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1412.5335.pdf:pdf},
keywords = {a fatty acid,and dansyiphosphatidylserine,and two drugs,attached to two lipids,dansyl probe,dansylbenzocaine and,dansylphosphatidylet,dansylpropranoloi,dansylundecanoic acid,fluorescence,lipid bilayer,of the dansyl group,olamine,several methods for characterising,solvent relaxation,systems,the solvent relaxation properties,variety of different lipid,were compared in a},
mendeley-groups = {DeepLearning},
title = {{Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews}},
url = {http://arxiv.org/pdf/1412.5335v7.pdf},
year = {2015}
}

@article{IlyaSutskeverOriolVinyals2014,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/5346-sequence-to-sequence-learning-with-neural-networks.pdf:pdf},
journal = {NIPS},
mendeley-groups = {DeepLearning},
title = {{Sequence to Sequence Learning with Neural Networks}},
year = {2014}
}

@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting on association for computational linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}

@article{lavie2009meteor,
  title={The METEOR metric for automatic evaluation of machine translation},
  author={Lavie, Alon and Denkowski, Michael J},
  journal={Machine translation},
  volume={23},
  number={2-3},
  pages={105--115},
  year={2009},
  publisher={Springer}
}

@article{sordoni2015neural,
  title={A neural network approach to context-sensitive generation of conversational responses},
  author={Sordoni, Alessandro and Galley, Michel and Auli, Michael and Brockett, Chris and Ji, Yangfeng and Mitchell, Margaret and Nie, Jian-Yun and Gao, Jianfeng and Dolan, Bill},
  journal={arXiv preprint arXiv:1606.06714},
  year={2016}
}

@article{liu2009learning,
  title={Learning to rank for information retrieval},
  author={Liu, Tie-Yan},
  journal={Foundations and Trends in Information Retrieval},
  volume={3},
  number={3},
  pages={225--331},
  year={2009},
  publisher={Now Publishers Inc.}
}


@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/Bobby\_paper1.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
mendeley-groups = {DeepLearning},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}

@article{Schuster1997,
abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported},
author = {Schuster, M. and Paliwal, K. K},
doi = {10.1109/78.650093},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/ieeesp97\_schuster.pdf:pdf},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
mendeley-groups = {DeepLearning},
number = {11},
pages = {2673--2681},
title = {{Bidirectional recurrent neural networks}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=650093},
volume = {45},
year = {1997}
}

@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {1410.5401},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1410.5401v2.pdf:pdf},
mendeley-groups = {DeepLearning},
month = oct,
pages = {1--26},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}


@article{Sukhbaatar2015,
abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates slightly better performance than RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
eprint = {1503.08895},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1503.08895v4.pdf:pdf},
mendeley-groups = {DeepLearning},
pages = {1--11},
title = {{End-To-End Memory Networks}},
url = {http://arxiv.org/abs/1503.08895},
year = {2015}
}

@article{Joulin2015,
abstract = {Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01007v4},
author = {Joulin, Armand and Mikolov, Tomas},
eprint = {arXiv:1503.01007v4},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1503.01007v4.pdf:pdf},
mendeley-groups = {Dialog,DeepLearning},
title = {{Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets}},
year = {2015}
}

@article{Bahdanau2014,
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1409.0473},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1409.0473v6.pdf:pdf},
journal = {{International Conference on Learning Representations}},
keywords = {Neural machine translation is a recently proposed,Unlike the traditional statistical machine transla,a source sentence into a fixed-length vector from,and propose to extend this by allowing a model to,bottleneck in improving the performance of this ba,for parts of a source sentence that are relevant t,having to form these parts as a hard segment expli,machine translation often belong to a family of en,maximize the translation performance. The models p,phrase-based system on the task of English-to-Fren,qualitative analysis reveals that the (soft-)align,the neural machine,translation aims at building a single neural netwo,translation. In this paper,we achieve a translation performance comparable to,we conjecture that the use of a fixed-length vecto,well with our intuition,without},
mendeley-groups = {DeepLearning},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473v3},
year = {2015}
}


@article{Branavan2012,
abstract = {This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the of?cial game manual as the text guide. Our results show that a linguistically-informed game-playing agent signi?cantly outperforms its language-unaware counterpart, yielding a 27\% absolute improvement and winning over 78\% of games when playing against the builtin AI of Civilization II.},
author = {Branavan, S. R K and Silver, David and Barzilay, Regina},
doi = {10.1613/jair.3560},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/live-3484-6254-jair.pdf:pdf},
isbn = {9781932432879},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {621--659},
title = {{Learning to win by reading manuals in a monte-carlo framework}},
volume = {43},
year = {2012}
}

@article{@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
},
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{Gers2002,
abstract = {The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by "peephole connections" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.},
author = {Gers, Felix and Schraudolph, Nicol and Schmidhuber, Jurgen},
doi = {10.1162/153244303768966139},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/GersSS02.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {long short term memory,recurrent neural networks,timing},
mendeley-groups = {DeepLearning},
number = {1},
pages = {115--143},
pmid = {17272722},
title = {{Learning Precise Timing with LSTM Recurrent Networks}},
url = {http://www.crossref.org/jmlr{\_}DOI.html},
volume = {3},
year = {2002}
}

@article{Narasimhan2015a,
author = {Narasimhan, Karthik and Barzilay, Regina},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/MCDR15.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
mendeley-groups = {DeepLearning},
pages = {1253--1262},
title = {{Machine Comprehension with Discourse Relations}},
url = {http://www.aclweb.org/anthology/P15-1121},
year = {2015}
}


@article{Ferrucci2010,
abstract = {IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV Quiz show, Jeopardy! The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy! Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After 3 years of intense research and development by a core team of about 20 researches, Watson is performing at human expert-levels in terms of precision, confidence and speed at the Jeopardy! Quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that may be used as a foundation for combining, deploying, evaluating and advancing a wide range of algorithmic techniques to rapidly advance the field of QA.},
author = {Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya a. and Lally, Adam and Murdock, J. William and Nyberg, Eric and Prager, John and Schlaefer, Nico and Welty, Chris},
doi = {10.1609/aimag.v31i3.2303},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/2303-3398-1-PB.pdf:pdf},
isbn = {9781450301787},
issn = {0738-4602},
journal = {AI Magazine},
number = {3},
pages = {59--79},
title = {{Building Watson: An Overview of the DeepQA Project}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/view/2303},
volume = {31},
year = {2010}
}

@article{Sachan2015,
author = {Sachan, Mrinmaya and Dubey, Avinava and Xing, Eric P and Richardson, Matthew},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/AnswerEntailing{\_}ACL2015.pdf:pdf},
isbn = {9781941643723},
journal = {Association for Computational Linguistics (ACL)},
mendeley-groups = {DeepLearning},
pages = {239--249},
title = {{Learning Answer-Entailing Structures for Machine Comprehension}},
year = {2015}
}


@article{Richardson2013,
abstract = {We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.},
author = {Richardson, Matthew and Burges, Christopher J C and Renshaw, Erin},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/D13-1020.pdf:pdf},
isbn = {9781937284978},
journal = {Empirical Methods in Natural Language Processing (EMNLP)},
pages = {193--203},
title = {{MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text}},
year = {2013}
}

@Inbook{Powell1994,
author="Powell, Michael J. D.",
editor="Gomez, Susana
and Hennart, Jean-Pierre",
chapter="A Direct Search Optimization Method That Models the Objective and Constraint Functions by Linear Interpolation",
title="Advances in Optimization and Numerical Analysis",
year="1994",
publisher="Springer Netherlands",
address="Dordrecht",
pages="51--67",
isbn="978-94-015-8330-5",
doi="10.1007/978-94-015-8330-5_4",
url="http://dx.doi.org/10.1007/978-94-015-8330-5_4"
}

@article{Saxe2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6120v3},
author = {Saxe, Andrew M and Mcclelland, James L and Ganguli, Surya},
eprint = {arXiv:1312.6120v3},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1312.6120v3.pdf:pdf},
journal = {International Conference on Learning Representations},
mendeley-groups = {DeepLearning},
title = {{Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}},
year = {2014}
}

@inproceedings{vinyals2015pointer,
  title={Pointer networks},
  author={Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2674--2682},
  year={2015}
}


@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
eprint = {arXiv:1211.5063v2},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1211.5063v2.pdf:pdf},
journal = {Proceedings of The 30th International Conference on Machine Learning},
mendeley-groups = {DeepLearning},
pages = {1310--1318},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}


@article{Kumar2015,
abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant an-swers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on sev-eral types of tasks and datasets: question answering (Facebook's bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclu-sively on trained word vector representations and requires no string matching or manually engineered features.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.07285v1},
author = {Kumar, Ankit and Irsoy, Ozan and Su, Jonathan and Bradbury, James and English, Robert and Pierce, Brian and Ondruska, Peter and Gulrajani, Ishaan and Socher, Richard},
eprint = {arXiv:1506.07285v1},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1506.07285v1.pdf:pdf},
mendeley-groups = {DeepLearning},
pages = {1--10},
title = {{Ask Me Anything: Dynamic Memory Networks for Natural Language Processing}},
year = {2015}
}


@article{Yu2015a,
abstract = {In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation, Neural Turing Machine, and Memory Networks for a simulated QA data set. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.},
archivePrefix = {arXiv},
arxivId = {1510.07526},
author = {Yu, Yang and Zhang, Wei and Hang, Chung-Wei and Zhou, Bowen},
eprint = {1510.07526},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1510.07526v2.pdf:pdf},
mendeley-groups = {DeepLearning},
pages = {1--5},
title = {{Empirical Study on Deep Learning Models for QA}},
url = {http://arxiv.org/abs/1510.07526},
year = {2015}
}

 eds J.M. Chambers and T.J. Hastie, Wadsworth & Brooks/Cole.

@incollection{Cleveland1992,
  author       = {William S. Cleveland and Eric Grosse and W. M. Shyu}, 
  title        = {Local regression models},
  chapter      = 8,
  publisher    = {Wadsworth & Brooks/Cole},
  year         = 1992,
  address      = {Pacific Grove, Calif.},
  editor       = { John M. Chambers and Trevor J. Hastie}
}

@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555v1},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1412.3555v1},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1412.3555v1(1).pdf:pdf},
journal = {arXiv},
mendeley-groups = {DeepLearning},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
year = {2014}
}

@article{Kobayashi2016,
author = {Kobayashi, Sosuke and Tian, Ran and Okazaki, Naoaki and Inui, Kentaro},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kobayashi et al. - 2016 - Dynamic Entity Representation with Max-pooling Improves Machine Reading.pdf:pdf},
journal = {Proceedings of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL-HLT)},
mendeley-groups = {DeepLearning},
title = {{Dynamic Entity Representation with Max-pooling Improves Machine Reading}},
year = {2016}
}

@inproceedings{chen2016thorough,
title={{A Thorough Examination of the CNN / Daily Mail Reading Comprehension Task}},
author={Chen, Danqi and Bolton, Jason and Manning, Christopher D.},
year={2016},
booktitle = {ACL}
}

@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different �thinned� networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/srivastava14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
mendeley-groups = {DeepLearning},
pages = {1929--1958},
title = {{Dropout: prevent NN from overfitting}},
volume = {15},
year = {2014}
}

@article{Cui2016a,
abstract = {Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading compre-hension datasets, which consist of People Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we propose a consensus attention-based neural network architecture to tackle the Cloze-style reading comprehension problem, which aims to induce a consensus attention over every words in the query. Experimental results show that the proposed neural network signif-icantly outperforms the state-of-the-art baselines in several public datasets. Furthermore, we setup a baseline for Chinese reading comprehension task, and hopefully this would speed up the process for future research.},
archivePrefix = {arXiv},
arxivId = {1607.02250},
author = {Cui, Yiming and Liu, Ting and Chen, Zhipeng and Wang, Shijin and Hu, Guoping},
eprint = {1607.02250},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1607.02250v2.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Consensus Attention-based Neural Networks for Chinese Reading Comprehension}},
year = {2016}
}

@inproceedings{Cui2016,
abstract = {Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this work, we present a novel model for Cloze-style reading comprehension tasks, called attention-over-attention reader. Our model aims to place another attention mechanism over the document-level attention, and induces "attended attention" for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test datasets.},
archivePrefix = {arXiv},
booktitle = {ACL},
arxivId = {1607.04423},
author = {Cui, Yiming and Chen, Zhipeng and Wei, Si and Wang, Shijin and Liu, Ting and Hu, Guoping},
eprint = {1607.04423},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1607.04423.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Attention-over-Attention Neural Networks for Reading Comprehension}},
year = {2017}
}


@article{Sordoni2016,
abstract = {We propose a novel neural attention architec-ture to tackle machine comprehension tasks, such as answering Cloze-style queries with re-spect to a document. Unlike previous models, we do not collapse the query into a single vec-tor, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the docu-ment. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.},
archivePrefix = {arXiv},
arxivId = {1606.02245},
author = {Sordoni, Alessandro and Bachman, Phillip and Bengio, Yoshua},
eprint = {1606.02245},
mendeley-groups = {DeepLearning},
title = {{Iterative Alternating Neural Attention for Machine Reading}},
year = {2016}
}

@inproceedings{Dhingra2016,
abstract = {In this paper we study the problem of answering cloze-style questions over short documents. We introduce a new attention mechanism which uses multiplicative interactions between the query embedding and intermediate states of a recurrent neural network reader. This enables the reader to build query-specific representations of tokens in the document which are further used for answer selection. Our model, the Gated-Attention Reader, outperforms all state-of-the-art models on several large-scale benchmark datasets for this task---the CNN $\backslash${\&} Dailymail news stories and Children's Book Test. We also provide a detailed analysis of the performance of our model and several baselines over a subset of questions manually annotated with certain linguistic features. The analysis sheds light on the strengths and weaknesses of several existing models.},
archivePrefix = {arXiv},
arxivId = {1606.01549},
booktitle = {ACL},
author = {Dhingra, Bhuwan and Liu, Hanxiao and Cohen, William W. and Salakhutdinov, Ruslan},
title = {{Gated-Attention Readers for Text Comprehension}},
year = {2017}
}

@inproceedings{Trischler2016a,
abstract = {We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children's Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1606.02270},
author = {Trischler, Adam and Ye, Zheng and Yuan, Xingdi and Suleman, Kaheer},
eprint = {1606.02270},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1606.02270.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Natural Language Comprehension with the EpiReader}},
booktitle={EMNLP},
year = {2016}
}

@article{Weissenborn2016,
abstract = {We present a novel neural architecture for answering queries, designed to optimally leverage explicit support in the form of query-answer memories. Our model is able to refine and update a given query while separately accumulating evidence for predicting the answer. Its architecture reflects this separation with dedicated embedding matrices and loosely connected information pathways (modules) for updating the query and accumulating evidence. This separation of responsibilities effectively decouples the search for query related support and the prediction of the answer. On recent benchmark datasets for reading comprehension, our model achieves state-of-the-art results. A qualitative analysis demonstrates that the model effectively accumulates weighted evidence from the query and over multiple support retrieval cycles which results in a robust answer prediction.},
archivePrefix = {arXiv},
arxivId = {1607.03316},
author = {Weissenborn, Dirk},
eprint = {1607.03316},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1607.03316.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Separating Answers from Queries for Neural Reading Comprehension}},
url = {http://arxiv.org/abs/1607.03316},
year = {2016}
}

@inproceedings{Kadlec2016,
archivePrefix = {arXiv},
arxivId = {1603.01547},
author = {Kadlec, Rudolf and Schmid, Martin and Bajgar, Ondrej and Kleindienst, Jan},
eprint = {1603.01547},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1603.01547v1.pdf:pdf},
booktitle = {ACL},
title = {{Text Understanding with the Attention Sum Reader Network}},
year = {2016}
}


@article{Paperno2016,
archivePrefix = {arXiv},
arxivId = {1606.06031},
author = {Paperno, Denis and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern, Raquel},
eprint = {1606.06031},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paperno et al. - Unknown - The LAMBADA dataset Word prediction requiring a broad discourse context},
journal = {Proceedings of ACL},
mendeley-groups = {DeepLearning,Dialog},
title = {{The LAMBADA dataset : Word prediction requiring a broad discourse context}},
year = {2016}
}


@article{Clark2015,
abstract = {While there has been an explosion of impressive, data-driven AI applications in recent years, machines still largely lack a deeper understanding of the world to answer questions that go beyond information explicitly stated in text, and to explain and discuss those answers. To reach this next generation of AI applications, it is imperative to make faster progress in areas of knowledge, modeling, reasoning, and language. Standardized tests have often been proposed as a driver for such progress, with good reason: Many of the questions require sophisticated understanding of both language and the world, pushing the boundaries of AI, while other questions are easier, supporting incremental progress. In Project Aristo at the Allen Institute for AI, we are working on a specific version of this challenge, namely having the computer pass Elementary School Science and Math exams. Even at this level there is a rich variety of problems and question types, the most difficult requiring significant progress in AI. Here we propose this task as a challenge problem for the community, and are providing supporting datasets. Solutions to many of these problems would have a major impact on the field so we encourage you: Take the Aristo Challenge!},
author = {Clark, Peter},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/aristo-challenge.pdf:pdf},
isbn = {9781577357032},
journal = {Proceedings of IAAI},
keywords = {Challenge Problem Track},
pages = {4019--4021},
title = {{Elementary school science and math tests as a driver for AI: Take the Aristo Challenge}},
year = {2015}
}


@article{Schoenick2016,
archivePrefix = {arXiv},
arxivId = {1604.04315},
author = {Schoenick, Carissa and Clark, Peter and Tafjord, Oyvind and Turney, Peter and Etzioni, Oren},
eprint = {1604.04315},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1604.04315.pdf:pdf},
journal = {ArXiV},
number = {April},
title = {{Moving Beyond the Turing Test with the Allen AI Science Challenge}},
year = {2016}
}


@article{Jelinek2004,
author = {Jelinek, Frederick},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/jelinek.pdf:pdf},
title = {{Some of my Best Friends are Linguists Applying Information Theoretic Methods : Evaluation of Grammar Quality}},
year = {2004}
}

@article{Halevy2009,
author = {Alon Halevy and Peter Norvig and Fernando Pereira},
title = {The Unreasonable Effectiveness of Data},
journal ={IEEE Intelligent Systems},
volume = {24},
number = {2},
issn = {1541-1672},
year = {2009},
pages = {8-12},
doi = {http://doi.ieeecomputersociety.org/10.1109/MIS.2009.36},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}


@article{Banko2001,
abstract = {The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.},
author = {Banko, Michele and Brill, Eric},
doi = {10.3115/1073012.1073017},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/P01-1005.pdf:pdf},
journal = {ACL '01 Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
pages = {26--33},
pmid = {1000106270},
title = {{Scaling to Very Very Large Corpora for Natural Language Disambiguation}},
url = {http://portal.acm.org/citation.cfm?doid=1073012.1073017},
year = {2001}
}


@article{Rajpurkar2016,
abstract = {We present a new reading comprehension dataset, SQuAD, consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset in both manual and automatic ways to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We built a strong logistic regression model, which achieves an F1 score of 51.0{\%}, a significant improvement over a simple baseline (20{\%}). However, human performance (86.8{\%}) is much higher, indicating that the dataset presents a good challenge problem for future research.},
archivePrefix = {arXiv},
arxivId = {1606.05250},
author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
eprint = {1606.05250},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rajpurkar et al. - 2016 - SQuAD 100,000 Questions for Machine Comprehension of Text.pdf:pdf},
journal = {EMNLP},
mendeley-groups = {DeepLearning},
title = {{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},
url = {http://arxiv.org/abs/1606.05250},
year = {2016}
}



@article{Zhu2015,
author = {Zhu, Yukun and Kiros, Ryan and Zemel, Richard and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
file = {:Users/rudolfkadlec/Downloads/Zhu{\_}Aligning{\_}Books{\_}and{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
journal = {Proceedings of ICCV},
mendeley-groups = {DeepLearning},
pages = {19--27},
title = {{Zhu{\_}Aligning{\_}Books{\_}and{\_}ICCV{\_}2015{\_}paper}},
year = {2015}
}

@article{Finkel2005,
abstract = {Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9{\%} over state-of-the-art systems on two established information extraction tasks.},
author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher},
doi = {10.3115/1219840.1219885},
file = {:Users/rudolfkadlec/Downloads/FinkelGM05.pdf:pdf},
issn = {02773791},
journal = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics ACL 05},
number = {June},
pages = {363--370},
title = {{Incorporating non-local information into information extraction systems by Gibbs sampling}},
url = {http://portal.acm.org/citation.cfm?doid=1219840.1219885},
volume = {43},
year = {2005}
}


@article{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ?ne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24{\%} accuracy on the Penn Treebank WSJ, an error reduction of 4.4{\%} on the best previous single automatically learned tagging result},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D},
doi = {10.3115/1073445.1073478},
file = {:Users/rudolfkadlec/Downloads/tagging.pdf:pdf},
journal = {In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL '03),},
pages = {252--259},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://dl.acm.org/citation.cfm?id=1073478},
year = {2003}
}


@article{Trischler2016,
abstract = {Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging {\{}$\backslash$it MCTest{\}} benchmark. Partly because of its limited size, prior work on {\{}$\backslash$it MCTest{\}} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for {\{}$\backslash$it MCTest{\}}, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15$\backslash${\%} absolute).},
archivePrefix = {arXiv},
arxivId = {1603.08884},
author = {Trischler, Adam and Ye, Zheng and Yuan, Xingdi and He, Jing and Bachman, Phillip and Suleman, Kaheer},
eprint = {1603.08884},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1603.08884v1.pdf:pdf},
journal = {Proceedings of ACL},
mendeley-groups = {DeepLearning},
title = {{A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data}},
url = {http://arxiv.org/abs/1603.08884},
year = {2016}
}


@article{Li2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1607.06275v1},
author = {Li, Peng and Li, Wei and He, Zhengyan and Wang, Xuguang and Cao, Ying and Zhou, Jie and Xu, Wei},
eprint = {arXiv:1607.06275v1},
file = {:Users/rudolfkadlec/Downloads/1607.06275.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering}},
year = {2016}
}


@article{Santos2016,
abstract = {In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.03609},
author = {dos Santos, Cicero and Tan, Ming and Xiang, Bing and Zhou, Bowen},
eprint = {1602.03609},
file = {:Users/rudolfkadlec/Library/Application Support/Mendeley Desktop/Downloaded/Santos et al. - 2016 - Attentive Pooling Networks.pdf:pdf},
mendeley-groups = {DeepLearning},
number = {Cv},
title = {{Attentive Pooling Networks}},
url = {http://arxiv.org/abs/1602.03609},
year = {2016}
}


@article{Mostafazadeh2016,
abstract = {Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of {\~{}}50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.},
archivePrefix = {arXiv},
arxivId = {1604.01696},
author = {Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Devi and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James},
eprint = {1604.01696},
file = {:Users/rudolfkadlec/Downloads/1604.01696v1.pdf:pdf},
journal = {Proceedings of NAACL},
title = {{A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories}},
url = {http://arxiv.org/abs/1604.01696},
year = {2016}
}

@article{Hewlett2016,
author = {Hewlett, Daniel and Lacoste, Alexandre and Jones, Llion and Polosukhin, Illia and Fandrianto, Andrew and Han, Jay and Kelcey, Matthew and Berthelot, David},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Desktop/Dropbox/Good{\_}Reader/Papers/Datasets/google16{\_}WikiReading.pdf:pdf},
journal = {Acl 2016},
pages = {1535--1545},
title = {{WIKI READING : A Novel Large-scale Language Understanding Task over Wikipedia}},
year = {2016}
}


@inproceedings{NELL-aaai15, Title = {Never-Ending Learning}, Author = {T. Mitchell and W. Cohen and E. Hruschka and P. Talukdar and J. Betteridge and A. Carlson and B. Dalvi and M. Gardner and B. Kisiel and J. Krishnamurthy and N. Lao and K. Mazaitis and T. Mohamed and N. Nakashole and E. Platanios and A. Ritter and M. Samadi and B. Settles and R. Wang and D. Wijaya and A. Gupta and X. Chen and A. Saparov and M. Greaves and J. Welling}, Booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)}, Year = {2015}} 

@article{Kawakami2015,
abstract = {We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these.},
archivePrefix = {arXiv},
arxivId = {1511.04623},
author = {Kawakami, Kazuya and Dyer, Chris},
eprint = {1511.04623},
file = {:Users/rudolfkadlec/Downloads/1511.04623v2.pdf:pdf},
journal = {ICLR Workshop Track},
mendeley-groups = {DeepLearning},
title = {{Learning to Represent Words in Context with Multilingual Supervision}},
url = {http://arxiv.org/abs/1511.04623},
year = {2016}
}


@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:Users/rudolfkadlec/Downloads/1301.3781.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}


@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:Users/rudolfkadlec/Library/Application Support/Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - GloVe Global Vectors for Word Representation.pdf:pdf},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {DeepLearning},
pages = {1532--1543},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}

@article{Onishi2016,
abstract = {We have constructed a new "Who-did-What" dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles --- an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization --- each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84{\%} solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.},
archivePrefix = {arXiv},
arxivId = {1608.05457},
author = {Onishi, Takeshi and Wang, Hai and Bansal, Mohit and Gimpel, Kevin and McAllester, David},
eprint = {1608.05457},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Desktop/Dropbox/Good{\_}Reader/Papers/QA/toyota16{\_}WhoDidWhat{\_}LargeScalePersonCenteredClozeDataset.pdf:pdf},
number = {3},
title = {{Who did What: A Large-Scale Person-Centered Cloze Dataset}},
url = {http://arxiv.org/abs/1608.05457},
year = {2016}
}

@article{Shen2016,
abstract = {Teaching a computer to read a document and answer general questions pertaining to the document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called Reasoning Network ({\{}ReasoNet{\}}) for machine comprehension tasks. ReasoNet makes use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNet introduces a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNet can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNet has achieved state-of-the-art performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, and a structured Graph Reachability dataset.},
archivePrefix = {arXiv},
arxivId = {1609.05284},
author = {Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu},
eprint = {1609.05284},
title = {{ReasoNet: Learning to Stop Reading in Machine Comprehension}},
url = {http://arxiv.org/abs/1609.05284},
year = {2016}
}


@article{Nguyen2016,
author = {Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/3ce987396a42538babbc77ac4afbf1e44815be13.pdf:pdf},
journal = {submitted to ICLR 2017},
mendeley-groups = {DeepLearning/ICLR 2017},
title = {{MS MARCO: A HUMAN GENERATED MACHINE READING COMPREHENSION DATASET}},
year = {2016}
}

@article{Trischler2016c,
author = {Trischler, Adam and Wang, Tong and Yuan, Xingdi and Harris, Justin and Sordoni, Alessandro and Bachman, Philip and Suleman, Kaheer},
file = {:Users/rudolfkadlec/Library/Application Support/Mendeley Desktop/Downloaded/Sordoni, Harris, Bachman - 2017 - NEWSQA DATASET.pdf:pdf},
mendeley-groups = {DeepLearning/ICLR 2017},
title = {{NEWSQA DATASET}},
journal = {submitted to ICLR 2017},
year = {2016}
}


@article{Munkhdalai2016,
abstract = {Hypothesis testing is an important cognitive process that supports human reasoning. In this paper, we introduce a computational hypothesis testing approach based on memory augmented neural networks. Our approach involves a hypothesis testing loop that reconsiders and progressively refines a previously formed hypothesis in order to generate new hypotheses to test. We apply the proposed approach to language comprehension task by using Neural Semantic Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an absolute improvement of 1.2{\%} to 2.6{\%} accuracy over previous results obtained by single and ensemble systems on standard machine comprehension benchmarks such as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.},
author = {Munkhdalai, Tsendsuren and Yu, Hong},
title = {{Reasoning with Memory Augmented Neural Networks for Language Comprehension}},
year = {2017}
}


@article{Levesque2014,
abstract = {The science of AI is concerned with the study of intelligent forms of behaviour in computational terms. But what does it tell us when a good semblance of a behaviour can be achieved using cheap tricks that seem to have little to do with what we intuitively imagine intelligence to be? Are these intuitions wrong, and is intelligence really just a bag of tricks? Or are the philosophers right, and is a behavioural understanding of intelligence simply too weak? I think both of these are wrong. I suggest in the context of question-answering that what matters when it comes to the science of AI is not a good semblance of intelligent behaviour at all, but the behaviour itself, what it depends on, and how it can be achieved. I go on to discuss two major hurdles that I believe will need to be cleared. {\textcopyright} 2014 Elsevier B.V.},
author = {Levesque, Hector J.},
doi = {10.1016/j.artint.2014.03.007},
file = {:Users/rudolfkadlec/Library/Application Support/Mendeley Desktop/Downloaded/Levesque - 2014 - On our best behaviour.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {IJCAI Research Excellence},
number = {1},
pages = {27--32},
pmid = {966205},
title = {{On our best behaviour}},
volume = {212},
year = {2014}
}


@article{Wang2016,
abstract = {Reading comprehension is a question answering task where the answer is to be found in a given passage about entities and events not mentioned in general knowledge sources. A significant number of neural architectures for this task (neural readers) have recently been developed and evaluated on large cloze-style datasets. We present experiments supporting the existence of logical structure in the hidden state vectors of "aggregation readers" such as the Attentive Reader and Stanford Reader. The logical structure of aggregation readers reflects the architecture of "explicit reference readers" such as the Attention-Sum Reader, the Gated Attention Reader and the Attention-over-Attention Reader. This relationship between aggregation readers and explicit reference readers presents a case study in emergent logical structure. In an independent contribution, we show that the addition of linguistics features to the input to existing neural readers significantly boosts performance yielding the best results to date on the Who-did-What datasets.},
archivePrefix = {arXiv},
arxivId = {1611.07954},
author = {Wang, Hai and Onishi, Takeshi and Gimpel, Kevin and McAllester, David},
eprint = {1611.07954},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2016 - Emergent Logical Structure in Vector Representations of Neural Readers.pdf:pdf},
journal = {submitted to ICLR 2017},
mendeley-groups = {DeepLearning},
title = {{Emergent Logical Structure in Vector Representations of Neural Readers}},
url = {http://arxiv.org/abs/1611.07954},
year = {2016}
}

@article{Hall91,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2532163},
 abstract = {Two guidelines for nonparametric bootstrap hypothesis testing are highlighted. The first recommends that resampling be done in a way that reflects the null hypothesis, even when the true hypothesis is distant from the null. The second guideline argues that bootstrap hypothesis tests should employ methods that are already recognized as having good features in the closely related problem of confidence interval construction. Violation of the first guideline can seriously reduce the power of a test. Sometimes this reduction is spectacular, since it is most serious when the null hypothesis is grossly in error. The second guideline is of some importance when the conclusion of a test is equivocal. It has no direct bearing on power, but improves the level accuracy of a test.},
 author = {Peter Hall and Susan R. Wilson},
 journal = {Biometrics},
 number = {2},
 pages = {757-762},
 publisher = {[Wiley, International Biometric Society]},
 title = {Two Guidelines for Bootstrap Hypothesis Testing},
 volume = {47},
 year = {1991}
}

@article{Efron79,
  title={Bootstrap Methods: Another Look at the Jackknife},
  author={Efron, B},
  journal={The Annals of Statistics},
  pages={1--26},
  year={1979},
  publisher={JSTOR}
}

@article{Church2017,
abstract = {{\textless}p{\textgreater}There has been a trend for publications to report better and better numbers, but less and less insight. The literature is turning into a giant leaderboard, where publication depends on numbers and little else (such as insight and explanation). It is considered a feature that machine learning has become so powerful (and so opaque) that it is no longer necessary (or even relevant) to talk about how it works. Insight is not only not required any more, but perhaps, insight is no longer even considered desirable.{\textless}/p{\textgreater}},
author = {Church, Kenneth Ward},
journal = {Natural Language Engineering},
number = {03},
pages = {473--480},
title = {{Emerging trends: I did it, I did it, I did it, but \dots}},
volume = {23},
year = {2017}
}

@article{Student1908,
  title={The probable error of a mean},
  author={Student},
  journal={Biometrika},
  pages={1--25},
  year={1908},
  publisher={JSTOR}
}
	
@inproceedings{Chernick07,
title = {Confidence Sets and Hypothesis Testing},
author = {Chernick, Michael R.},
publisher = {John Wiley \& Sons, Inc.},
isbn = {9780470192573},
url = {http://dx.doi.org/10.1002/9780470192573.ch3},
doi = {10.1002/9780470192573.ch3},
pages = {53--77},
keywords = {confidence sets and hypothesis testing, bootstrap iteration (or double bootstrap), bootstrap CI and dose�response relationship},
booktitle = {Bootstrap Methods},
year = {2007},
}

@article{Mackinnon09,
  title={Bootstrap hypothesis testing},
  author={MacKinnon, James G},
  journal={Handbook of Computational Econometrics},
  pages={183--213},
  year={2009},
  publisher={John Wiley \& Sons, Ltd. Chichester, UK}
}

@article{Beran88,
 ISSN = {01621459},
 URL = {http://0-www.jstor.org.pugwash.lib.warwick.ac.uk/stable/2289292},
 abstract = {Approximate tests for a composite null hypothesis about a parameter ? may be obtained by referring a test statistic to an estimated critical value. Either asymptotic theory or bootstrap methods can be used to estimate the desired quantile. The simple asymptotic test f<sub>A</sub> refers the test statistic to a quantile of its asymptotic null distribution after unknown parameters have been estimated. The bootstrap approach used here is based on the concept of prepivoting. Prepivoting is the transformation of a test statistic by the cdf of its bootstrap null distribution. The simple bootstrap test f<sub>B</sub> refers the prepivoted test statistic to a quantile of the uniform (0, 1) distribution. Under regularity conditions, the bootstrap test f<sub>B</sub> has a smaller asymptotic order of error in level than does the asymptotic test f<sub>A</sub>, provided that the asymptotic null distribution of the test statistic does not depend on unknown parameters. In the contrary case, both f<sub>A</sub> and f<sub>B</sub> have the same order of level error. Certain classical refinements to asymptotic tests can be regarded as analytical approximations to the bootstrap test f<sub>B</sub>. These classical results include Welch's estimated t distribution solution to the Behrens-Fisher problem, Bartlett's adjustment to likelihood ratio tests, and Edgeworth expansion corrections to the nonparametric t test for a mean. On the other hand, the bootstrap test f<sub>B</sub> can also be approximated directly by a Monte Carlo algorithm. The prepivoted bootstrap test f<sub>B,1</sub> is obtained by prepivoting the test statistic twice before referring the result to a quantile of the uniform (0, 1) distribution. Under regularity conditions, the level error of f<sub>B,1</sub> is of smaller asymptotic order than is the level error of either f<sub>B</sub> or f<sub>A</sub>. Analytical and semianalytical approximations sometimes exist for f<sub>B,1</sub>. In general, a nested double-bootstrap Monte Carlo algorithm yields a satisfactory approximation to f<sub>B,1</sub>. The possibility of direct nonanalytical implementation is a great practical merit of both f<sub>B</sub> and f<sub>B,1</sub>.},
 author = {Rudolf Beran},
 journal = {Journal of the American Statistical Association},
 number = {403},
 pages = {687-697},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Prepivoting Test Statistics: A Bootstrap View of Asymptotic Refinements},
 volume = {83},
 year = {1988}
}


@article{Beran87,
 ISSN = {00063444},
 URL = {http://0-www.jstor.org.pugwash.lib.warwick.ac.uk/stable/2336685},
 abstract = {Approximate confidence sets for a parameter θ may be obtained by referring a function of θ and of the sample to an estimated quantile of that function's sampling distribution. We call this function the root of the confidence set. Either asymptotic theory of bootstrap methods can be used to estimate the desired quantile. When the root is not a pivot, in the sense of classical statistics, the actual level of the approximate confidence set may differ substantially from the intended level. Prepivoting is the transformation of a confidence set root by its estimated bootstrap cumulative distribution function. Prepivoting can be iterated. Bootstrap confidence sets generated from a root prepivoted one or more times have smaller error in level than do confidence sets based on the original root. The first prepivoting is nearly equivalent to studentizing, when that operation is appropriate. Further iterations of prepivoting make higher order corrections automatically.},
 author = {Rudolf Beran},
 journal = {Biometrika},
 number = {3},
 pages = {457-468},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Prepivoting to Reduce Level Error of Confidence Sets},
 volume = {74},
 year = {1987}
}

@article{Bergsra12,
  title={Random search for hyper-parameter optimization},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Feb},
  pages={281--305},
  year={2012}
}

@inproceedings{vanrijn13,
  title={OpenML: A collaborative science platform},
  author={Van Rijn, Jan N and Bischl, Bernd and Torgo, Luis and Gao, Bo and Umaashankar, Venkatesh and Fischer, Simon and Winter, Patrick and Wiswedel, Bernd and Berthold, Michael R and Vanschoren, Joaquin},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={645--649},
  year={2013},
  organization={Springer}
}

@book{adams1995hitchhiker,
  title={The Hitchhiker's Guide to the Galaxy},
  author={Adams, D.},
  isbn={9781417642595},
  url={http://books.google.com/books?id=W-xMPgAACAAJ},
  year={1995},
  publisher={San Val}
}


@article{Chelba2014,
abstract = {We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned Kneser-Ney 5-gram model achieves perplexity 67.6; a combination of techniques leads to 35{\%} reduction in perplexity, or 10{\%} reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.},
archivePrefix = {arXiv},
arxivId = {1312.3005},
author = {Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},
eprint = {1312.3005},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/41880.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Benchmark,Language modeling,Reproducible research},
mendeley-groups = {DeepLearning},
pages = {2635--2639},
title = {{One billion word benchmark for measuring progress in statistical language modeling}},
year = {2014}
}


@article{Jozefowicz2016,
abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.},
archivePrefix = {arXiv},
arxivId = {1602.02410},
author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
eprint = {1602.02410},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1602.02410.pdf:pdf},
journal = {International Conference on Machine Learning (ICML)},
mendeley-groups = {DeepLearning},
title = {{Exploring the Limits of Language Modeling}},
url = {http://arxiv.org/abs/1602.02410},
year = {2016}
}


@article{hill2015goldilocks,
  title={The Goldilocks Principle: Reading Children's Books with Explicit Memory Representations},
  author={Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
  journal={arXiv preprint arXiv:1511.02301},
  year={2015}
}

@article{weston2014memory,
  title={Memory networks},
  author={Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  journal={arXiv preprint arXiv:1410.3916},
  year={2014}
}

@inproceedings{hermann2015teaching,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1684--1692},
  year={2015}
}


@article{taylor1953cloze,
  title={Cloze procedure: a new tool for measuring readability.},
  author={Taylor, Wilson L},
  journal={Journalism and Mass Communication Quarterly},
  volume={30},
  number={4},
  pages={415},
  year={1953},
  publisher={Association for Education in Journalism, etc.}
}

@article{Cho2014,
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
eprint = {1406.1078},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1406.1078v3(1).pdf:pdf},
journal = {Empirical Methods in Natural Language Processing (EMNLP)},
mendeley-groups = {DeepLearning},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078v3},
year = {2014}
}


@article{lowe2015ubuntu,
  title={The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems},
  author={Lowe, Ryan and Pow, Nissan and Serban, Iulian and Pineau, Joelle},
  journal={arXiv preprint arXiv:1506.08909},
  year={2015}
}

@article{shang2015neural,
  title={Neural Responding Machine for Short-Text Conversation},
  author={Shang, Lifeng and Lu, Zhengdong and Li, Hang},
  journal={arXiv preprint arXiv:1503.02364},
  year={2015}
}

@article{vinyals2015neural,
  title={A Neural Conversational Model},
  author={Vinyals, Oriol and Le, Quoc},
  journal={arXiv preprint arXiv:1506.05869},
  year={2015}
}

@inproceedings{lai2015recurrent,
  title={Recurrent convolutional neural networks for text classification},
  author={Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
  booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}

@article{kim2014convolutional,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv preprint arXiv:1408.5882},
  year={2014}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}


@MISC{Bastien-Theano-2012,
        author = {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J. and Bergeron, Arnaud and Bouchard, Nicolas and Bengio, Yoshua},
         title = {Theano: new features and speed improvements},
          year = {2012},
  howpublished = {Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop}
}

@article{VanMerrienboer2015,
abstract = {We introduce two Python frameworks to train neural networks on large datasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler with CUDA-support. It facilitates the training of complex neural network models by providing parametrized Theano operations, attaching metadata to Theano's symbolic computational graph, and providing an extensive set of utilities to assist training the networks, e.g. training algorithms, logging, monitoring, visualization, and serialization. Fuel provides a standard format for machine learning datasets. It allows the user to easily iterate over large datasets, performing many types of pre-processing on the fly.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.00619v1},
author = {van Merrienboer, Bart and Bahdanau, Dzmitry and Dumoulin, Vincent and Serdyuk, Dmitriy and Warde-farley, David and Chorowski, Jan and Bengio, Yoshua},
eprint = {arXiv:1506.00619v1},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1506.00619v1.pdf:pdf},
keywords = {gpgpu,large-scale machine learning,neural networks},
mendeley-groups = {DeepLearning},
pages = {1--5},
title = {{Blocks and Fuel : Frameworks for deep learning}},
url = {http://arxiv.org/abs/1506.00619$\backslash$nhttps://github.com/mila-udem/blocks$\backslash$nhttps://github.com/mila-udem/fuel$\backslash$nhttps://github.com/mila-udem/blocks-extras$\backslash$nhttps://github.com/mila-udem/blocks-examples},
year = {2015}
}


@article{Pennington2014a,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75\% accuracy, an improvement of 11\% over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/glove.pdf:pdf},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {DeepLearning},
pages = {1532--1543},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}

@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v5},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v5},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1412.6980.pdf:pdf},
journal = {International Conference on Learning Representations},
mendeley-groups = {DeepLearning},
pages = {1--13},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}

@article{mesnil2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.5335v7},
author = {Mesnil, Gr\'{e}goire and Mikolov, Tomas and Ranzato, Marc'Aurelio and Bengio, Yoshua},
eprint = {arXiv:1412.5335v7},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1412.5335.pdf:pdf},
keywords = {a fatty acid,and dansyiphosphatidylserine,and two drugs,attached to two lipids,dansyl probe,dansylbenzocaine and,dansylphosphatidylet,dansylpropranoloi,dansylundecanoic acid,fluorescence,lipid bilayer,of the dansyl group,olamine,several methods for characterising,solvent relaxation,systems,the solvent relaxation properties,variety of different lipid,were compared in a},
mendeley-groups = {DeepLearning},
title = {{Ensemble of Generative and Discriminative Techniques for Sentiment Analysis of Movie Reviews}},
url = {http://arxiv.org/pdf/1412.5335v7.pdf},
year = {2015}
}

@article{IlyaSutskeverOriolVinyals2014,
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/5346-sequence-to-sequence-learning-with-neural-networks.pdf:pdf},
journal = {NIPS},
mendeley-groups = {DeepLearning},
title = {{Sequence to Sequence Learning with Neural Networks}},
year = {2014}
}

@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting on association for computational linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}

@article{lavie2009meteor,
  title={The METEOR metric for automatic evaluation of machine translation},
  author={Lavie, Alon and Denkowski, Michael J},
  journal={Machine translation},
  volume={23},
  number={2-3},
  pages={105--115},
  year={2009},
  publisher={Springer}
}

@article{sordoni2015neural,
  title={A neural network approach to context-sensitive generation of conversational responses},
  author={Sordoni, Alessandro and Galley, Michel and Auli, Michael and Brockett, Chris and Ji, Yangfeng and Mitchell, Margaret and Nie, Jian-Yun and Gao, Jianfeng and Dolan, Bill},
  journal={arXiv preprint arXiv:1606.06714},
  year={2016}
}

@article{liu2009learning,
  title={Learning to rank for information retrieval},
  author={Liu, Tie-Yan},
  journal={Foundations and Trends in Information Retrieval},
  volume={3},
  number={3},
  pages={225--331},
  year={2009},
  publisher={Now Publishers Inc.}
}


@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/Bobby\_paper1.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
mendeley-groups = {DeepLearning},
pages = {1735--1780},
pmid = {9377276},
title = {{Long Short-Term Memory}},
volume = {9},
year = {1997}
}

@article{Schuster1997,
abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported},
author = {Schuster, M. and Paliwal, K. K},
doi = {10.1109/78.650093},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/ieeesp97\_schuster.pdf:pdf},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
mendeley-groups = {DeepLearning},
number = {11},
pages = {2673--2681},
title = {{Bidirectional recurrent neural networks}},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=650093},
volume = {45},
year = {1997}
}

@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint = {1410.5401},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1410.5401v2.pdf:pdf},
mendeley-groups = {DeepLearning},
month = oct,
pages = {1--26},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}


@article{Sukhbaatar2015,
abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates slightly better performance than RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
archivePrefix = {arXiv},
arxivId = {1503.08895},
author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
eprint = {1503.08895},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1503.08895v4.pdf:pdf},
mendeley-groups = {DeepLearning},
pages = {1--11},
title = {{End-To-End Memory Networks}},
url = {http://arxiv.org/abs/1503.08895},
year = {2015}
}

@article{Joulin2015,
abstract = {Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.01007v4},
author = {Joulin, Armand and Mikolov, Tomas},
eprint = {arXiv:1503.01007v4},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1503.01007v4.pdf:pdf},
mendeley-groups = {Dialog,DeepLearning},
title = {{Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets}},
year = {2015}
}

@article{Bahdanau2014,
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1409.0473},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/1409.0473v6.pdf:pdf},
journal = {{International Conference on Learning Representations}},
keywords = {Neural machine translation is a recently proposed,Unlike the traditional statistical machine transla,a source sentence into a fixed-length vector from,and propose to extend this by allowing a model to,bottleneck in improving the performance of this ba,for parts of a source sentence that are relevant t,having to form these parts as a hard segment expli,machine translation often belong to a family of en,maximize the translation performance. The models p,phrase-based system on the task of English-to-Fren,qualitative analysis reveals that the (soft-)align,the neural machine,translation aims at building a single neural netwo,translation. In this paper,we achieve a translation performance comparable to,we conjecture that the use of a fixed-length vecto,well with our intuition,without},
mendeley-groups = {DeepLearning},
title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
url = {http://arxiv.org/abs/1409.0473v3},
year = {2015}
}


@article{Branavan2012,
abstract = {This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the ofﬁcial game manual as the text guide. Our results show that a linguistically-informed game-playing agent signiﬁcantly outperforms its language-unaware counterpart, yielding a 27\% absolute improvement and winning over 78\% of games when playing against the builtin AI of Civilization II.},
author = {Branavan, S. R K and Silver, David and Barzilay, Regina},
doi = {10.1613/jair.3560},
file = {:C$\backslash$:/Users/IBM\_ADMIN/Downloads/live-3484-6254-jair.pdf:pdf},
isbn = {9781932432879},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {621--659},
title = {{Learning to win by reading manuals in a monte-carlo framework}},
volume = {43},
year = {2012}
}

@article{@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
},
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@article{Gers2002,
abstract = {The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by "peephole connections" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals.},
author = {Gers, Felix and Schraudolph, Nicol and Schmidhuber, Jurgen},
doi = {10.1162/153244303768966139},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/GersSS02.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {long short term memory,recurrent neural networks,timing},
mendeley-groups = {DeepLearning},
number = {1},
pages = {115--143},
pmid = {17272722},
title = {{Learning Precise Timing with LSTM Recurrent Networks}},
url = {http://www.crossref.org/jmlr{\_}DOI.html},
volume = {3},
year = {2002}
}

@article{Narasimhan2015a,
author = {Narasimhan, Karthik and Barzilay, Regina},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/MCDR15.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
mendeley-groups = {DeepLearning},
pages = {1253--1262},
title = {{Machine Comprehension with Discourse Relations}},
url = {http://www.aclweb.org/anthology/P15-1121},
year = {2015}
}


@article{Ferrucci2010,
abstract = {IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV Quiz show, Jeopardy! The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy! Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After 3 years of intense research and development by a core team of about 20 researches, Watson is performing at human expert-levels in terms of precision, confidence and speed at the Jeopardy! Quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that may be used as a foundation for combining, deploying, evaluating and advancing a wide range of algorithmic techniques to rapidly advance the field of QA.},
author = {Ferrucci, David and Brown, Eric and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya a. and Lally, Adam and Murdock, J. William and Nyberg, Eric and Prager, John and Schlaefer, Nico and Welty, Chris},
doi = {10.1609/aimag.v31i3.2303},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/2303-3398-1-PB.pdf:pdf},
isbn = {9781450301787},
issn = {0738-4602},
journal = {AI Magazine},
number = {3},
pages = {59--79},
title = {{Building Watson: An Overview of the DeepQA Project}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/view/2303},
volume = {31},
year = {2010}
}

@article{Sachan2015,
author = {Sachan, Mrinmaya and Dubey, Avinava and Xing, Eric P and Richardson, Matthew},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/AnswerEntailing{\_}ACL2015.pdf:pdf},
isbn = {9781941643723},
journal = {Association for Computational Linguistics (ACL)},
mendeley-groups = {DeepLearning},
pages = {239--249},
title = {{Learning Answer-Entailing Structures for Machine Comprehension}},
year = {2015}
}


@article{Richardson2013,
abstract = {We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text. Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction). In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension. Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric. By being fictional, the answer typically can be found only in the story itself. The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task. We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions. By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost. By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.},
author = {Richardson, Matthew and Burges, Christopher J C and Renshaw, Erin},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/D13-1020.pdf:pdf},
isbn = {9781937284978},
journal = {Empirical Methods in Natural Language Processing (EMNLP)},
pages = {193--203},
title = {{MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text}},
year = {2013}
}

@Inbook{Powell1994,
author="Powell, Michael J. D.",
editor="Gomez, Susana
and Hennart, Jean-Pierre",
chapter="A Direct Search Optimization Method That Models the Objective and Constraint Functions by Linear Interpolation",
title="Advances in Optimization and Numerical Analysis",
year="1994",
publisher="Springer Netherlands",
address="Dordrecht",
pages="51--67",
isbn="978-94-015-8330-5",
doi="10.1007/978-94-015-8330-5_4",
url="http://dx.doi.org/10.1007/978-94-015-8330-5_4"
}

@article{Saxe2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6120v3},
author = {Saxe, Andrew M and Mcclelland, James L and Ganguli, Surya},
eprint = {arXiv:1312.6120v3},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1312.6120v3.pdf:pdf},
journal = {International Conference on Learning Representations},
mendeley-groups = {DeepLearning},
title = {{Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}},
year = {2014}
}

@inproceedings{vinyals2015pointer,
  title={Pointer networks},
  author={Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2674--2682},
  year={2015}
}


@article{Pascanu2012,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
eprint = {arXiv:1211.5063v2},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1211.5063v2.pdf:pdf},
journal = {Proceedings of The 30th International Conference on Machine Learning},
mendeley-groups = {DeepLearning},
pages = {1310--1318},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2012}
}


@article{Kumar2015,
abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant an-swers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on sev-eral types of tasks and datasets: question answering (Facebook's bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclu-sively on trained word vector representations and requires no string matching or manually engineered features.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.07285v1},
author = {Kumar, Ankit and Irsoy, Ozan and Su, Jonathan and Bradbury, James and English, Robert and Pierce, Brian and Ondruska, Peter and Gulrajani, Ishaan and Socher, Richard},
eprint = {arXiv:1506.07285v1},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1506.07285v1.pdf:pdf},
mendeley-groups = {DeepLearning},
pages = {1--10},
title = {{Ask Me Anything: Dynamic Memory Networks for Natural Language Processing}},
year = {2015}
}


@article{Yu2015a,
abstract = {In this paper we explore deep learning models with memory component or attention mechanism for question answering task. We combine and compare three models, Neural Machine Translation, Neural Turing Machine, and Memory Networks for a simulated QA data set. This paper is the first one that uses Neural Machine Translation and Neural Turing Machines for solving QA tasks. Our results suggest that the combination of attention and memory have potential to solve certain QA problem.},
archivePrefix = {arXiv},
arxivId = {1510.07526},
author = {Yu, Yang and Zhang, Wei and Hang, Chung-Wei and Zhou, Bowen},
eprint = {1510.07526},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1510.07526v2.pdf:pdf},
mendeley-groups = {DeepLearning},
pages = {1--5},
title = {{Empirical Study on Deep Learning Models for QA}},
url = {http://arxiv.org/abs/1510.07526},
year = {2015}
}

 eds J.M. Chambers and T.J. Hastie, Wadsworth & Brooks/Cole.

@incollection{Cleveland1992,
  author       = {William S. Cleveland and Eric Grosse and W. M. Shyu}, 
  title        = {Local regression models},
  chapter      = 8,
  publisher    = {Wadsworth & Brooks/Cole},
  year         = 1992,
  address      = {Pacific Grove, Calif.},
  editor       = { John M. Chambers and Trevor J. Hastie}
}

@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
archivePrefix = {arXiv},
arxivId = {1412.3555v1},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1412.3555v1},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1412.3555v1(1).pdf:pdf},
journal = {arXiv},
mendeley-groups = {DeepLearning},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
year = {2014}
}

@article{Kobayashi2016,
author = {Kobayashi, Sosuke and Tian, Ran and Okazaki, Naoaki and Inui, Kentaro},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kobayashi et al. - 2016 - Dynamic Entity Representation with Max-pooling Improves Machine Reading.pdf:pdf},
journal = {Proceedings of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL-HLT)},
mendeley-groups = {DeepLearning},
title = {{Dynamic Entity Representation with Max-pooling Improves Machine Reading}},
year = {2016}
}

@inproceedings{chen2016thorough,
title={{A Thorough Examination of the CNN / Daily Mail Reading Comprehension Task}},
author={Chen, Danqi and Bolton, Jason and Manning, Christopher D.},
year={2016},
booktitle = {Association for Computational Linguistics (ACL)}
}

@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/srivastava14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
mendeley-groups = {DeepLearning},
pages = {1929--1958},
title = {{Dropout: prevent NN from overfitting}},
volume = {15},
year = {2014}
}

@article{Cui2016a,
abstract = {Reading comprehension has embraced a booming in recent NLP research. Several institutes have released the Cloze-style reading comprehension data, and these have greatly accelerated the research of machine comprehension. In this work, we firstly present Chinese reading compre-hension datasets, which consist of People Daily news dataset and Children's Fairy Tale (CFT) dataset. Also, we propose a consensus attention-based neural network architecture to tackle the Cloze-style reading comprehension problem, which aims to induce a consensus attention over every words in the query. Experimental results show that the proposed neural network signif-icantly outperforms the state-of-the-art baselines in several public datasets. Furthermore, we setup a baseline for Chinese reading comprehension task, and hopefully this would speed up the process for future research.},
archivePrefix = {arXiv},
arxivId = {1607.02250},
author = {Cui, Yiming and Liu, Ting and Chen, Zhipeng and Wang, Shijin and Hu, Guoping},
eprint = {1607.02250},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1607.02250v2.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Consensus Attention-based Neural Networks for Chinese Reading Comprehension}},
year = {2016}
}

@article{Cui2016,
abstract = {Cloze-style queries are representative problems in reading comprehension. Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions. In this work, we present a novel model for Cloze-style reading comprehension tasks, called attention-over-attention reader. Our model aims to place another attention mechanism over the document-level attention, and induces "attended attention" for final predictions. Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling. Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test datasets.},
archivePrefix = {arXiv},
arxivId = {1607.04423},
author = {Cui, Yiming and Chen, Zhipeng and Wei, Si and Wang, Shijin and Liu, Ting and Hu, Guoping},
eprint = {1607.04423},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1607.04423.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Attention-over-Attention Neural Networks for Reading Comprehension}},
url = {http://arxiv.org/abs/1607.04423},
year = {2016}
}


@article{Sordoni2016,
abstract = {We propose a novel neural attention architec-ture to tackle machine comprehension tasks, such as answering Cloze-style queries with re-spect to a document. Unlike previous models, we do not collapse the query into a single vec-tor, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the docu-ment. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children's Book Test (CBT) dataset.},
archivePrefix = {arXiv},
arxivId = {1606.02245},
author = {Sordoni, Alessandro and Bachman, Phillip and Bengio, Yoshua},
eprint = {1606.02245},
mendeley-groups = {DeepLearning},
title = {{Iterative Alternating Neural Attention for Machine Reading}},
year = {2016}
}

@article{Dhingra2016,
abstract = {In this paper we study the problem of answering cloze-style questions over short documents. We introduce a new attention mechanism which uses multiplicative interactions between the query embedding and intermediate states of a recurrent neural network reader. This enables the reader to build query-specific representations of tokens in the document which are further used for answer selection. Our model, the Gated-Attention Reader, outperforms all state-of-the-art models on several large-scale benchmark datasets for this task---the CNN $\backslash${\&} Dailymail news stories and Children's Book Test. We also provide a detailed analysis of the performance of our model and several baselines over a subset of questions manually annotated with certain linguistic features. The analysis sheds light on the strengths and weaknesses of several existing models.},
archivePrefix = {arXiv},
arxivId = {1606.01549},
author = {Dhingra, Bhuwan and Liu, Hanxiao and Cohen, William W. and Salakhutdinov, Ruslan},
eprint = {1606.01549},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1606.01549.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Gated-Attention Readers for Text Comprehension}},
url = {http://arxiv.org/abs/1606.01549},
year = {2016}
}

@article{Trischler2016a,
abstract = {We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children's Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1606.02270},
author = {Trischler, Adam and Ye, Zheng and Yuan, Xingdi and Suleman, Kaheer},
eprint = {1606.02270},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1606.02270.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Natural Language Comprehension with the EpiReader}},
url = {http://arxiv.org/abs/1606.02270},
year = {2016}
}

@article{Weissenborn2016,
abstract = {We present a novel neural architecture for answering queries, designed to optimally leverage explicit support in the form of query-answer memories. Our model is able to refine and update a given query while separately accumulating evidence for predicting the answer. Its architecture reflects this separation with dedicated embedding matrices and loosely connected information pathways (modules) for updating the query and accumulating evidence. This separation of responsibilities effectively decouples the search for query related support and the prediction of the answer. On recent benchmark datasets for reading comprehension, our model achieves state-of-the-art results. A qualitative analysis demonstrates that the model effectively accumulates weighted evidence from the query and over multiple support retrieval cycles which results in a robust answer prediction.},
archivePrefix = {arXiv},
arxivId = {1607.03316},
author = {Weissenborn, Dirk},
eprint = {1607.03316},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1607.03316.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Separating Answers from Queries for Neural Reading Comprehension}},
url = {http://arxiv.org/abs/1607.03316},
year = {2016}
}

@article{Kadlec2016,
archivePrefix = {arXiv},
arxivId = {1603.01547},
author = {Kadlec, Rudolf and Schmid, Martin and Bajgar, Ondřej and Kleindienst, Jan},
eprint = {1603.01547},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1603.01547v1.pdf:pdf},
journal = {Proceedings of ACL},
keywords = {183-200,2,2013,39,abattage,accept{\'{e}} le 12 juin,animal management strategies,chalcolithic,chalcolithique,cnrs {\'{e}}ditions 2013,courbes d,kill-off patterns,levant sud,manuscrit re{\c{c}}u le 6,marj rabba,mars 2012,mots-cl{\'{e}}s,p,pal{\'{e}}orient,southern levant,strat{\'{e}}gies de gestion des,troupeaux,vol},
title = {{Neural Text Understanding with Attention Sum Reader}},
year = {2016}
}


@article{Paperno2016,
archivePrefix = {arXiv},
arxivId = {1606.06031},
author = {Paperno, Denis and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern, Raquel},
eprint = {1606.06031},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paperno et al. - Unknown - The LAMBADA dataset Word prediction requiring a broad discourse context},
journal = {Proceedings of ACL},
mendeley-groups = {DeepLearning,Dialog},
title = {{The LAMBADA dataset : Word prediction requiring a broad discourse context}},
year = {2016}
}


@article{Clark2015,
abstract = {While there has been an explosion of impressive, data-driven AI applications in recent years, machines still largely lack a deeper understanding of the world to answer questions that go beyond information explicitly stated in text, and to explain and discuss those answers. To reach this next generation of AI applications, it is imperative to make faster progress in areas of knowledge, modeling, reasoning, and language. Standardized tests have often been proposed as a driver for such progress, with good reason: Many of the questions require sophisticated understanding of both language and the world, pushing the boundaries of AI, while other questions are easier, supporting incremental progress. In Project Aristo at the Allen Institute for AI, we are working on a specific version of this challenge, namely having the computer pass Elementary School Science and Math exams. Even at this level there is a rich variety of problems and question types, the most difficult requiring significant progress in AI. Here we propose this task as a challenge problem for the community, and are providing supporting datasets. Solutions to many of these problems would have a major impact on the field so we encourage you: Take the Aristo Challenge!},
author = {Clark, Peter},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/aristo-challenge.pdf:pdf},
isbn = {9781577357032},
journal = {Proceedings of IAAI},
keywords = {Challenge Problem Track},
pages = {4019--4021},
title = {{Elementary school science and math tests as a driver for AI: Take the Aristo Challenge}},
year = {2015}
}


@article{Schoenick2016,
archivePrefix = {arXiv},
arxivId = {1604.04315},
author = {Schoenick, Carissa and Clark, Peter and Tafjord, Oyvind and Turney, Peter and Etzioni, Oren},
eprint = {1604.04315},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1604.04315.pdf:pdf},
journal = {ArXiV},
number = {April},
title = {{Moving Beyond the Turing Test with the Allen AI Science Challenge}},
year = {2016}
}


@article{Jelinek2004,
author = {Jelinek, Frederick},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/jelinek.pdf:pdf},
title = {{Some of my Best Friends are Linguists Applying Information Theoretic Methods : Evaluation of Grammar Quality}},
year = {2004}
}

@article{Halevy2009,
author = {Alon Halevy and Peter Norvig and Fernando Pereira},
title = {The Unreasonable Effectiveness of Data},
journal ={IEEE Intelligent Systems},
volume = {24},
number = {2},
issn = {1541-1672},
year = {2009},
pages = {8-12},
doi = {http://doi.ieeecomputersociety.org/10.1109/MIS.2009.36},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
}


@article{Banko2001,
abstract = {The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.},
author = {Banko, Michele and Brill, Eric},
doi = {10.3115/1073012.1073017},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/P01-1005.pdf:pdf},
journal = {ACL '01 Proceedings of the 39th Annual Meeting on Association for Computational Linguistics},
pages = {26--33},
pmid = {1000106270},
title = {{Scaling to Very Very Large Corpora for Natural Language Disambiguation}},
url = {http://portal.acm.org/citation.cfm?doid=1073012.1073017},
year = {2001}
}


@article{Rajpurkar2016,
abstract = {We present a new reading comprehension dataset, SQuAD, consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset in both manual and automatic ways to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We built a strong logistic regression model, which achieves an F1 score of 51.0{\%}, a significant improvement over a simple baseline (20{\%}). However, human performance (86.8{\%}) is much higher, indicating that the dataset presents a good challenge problem for future research.},
archivePrefix = {arXiv},
arxivId = {1606.05250},
author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
eprint = {1606.05250},
file = {:Users/rudolfkadlec/Downloads/1606.05250v1.pdf:pdf},
mendeley-groups = {DeepLearning},
number = {ii},
title = {{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},
url = {http://arxiv.org/abs/1606.05250},
year = {2016}
}


@article{Zhu2015,
author = {Zhu, Yukun and Kiros, Ryan and Zemel, Richard and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
file = {:Users/rudolfkadlec/Downloads/Zhu{\_}Aligning{\_}Books{\_}and{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
journal = {Proceedings of ICCV},
mendeley-groups = {DeepLearning},
pages = {19--27},
title = {{Zhu{\_}Aligning{\_}Books{\_}and{\_}ICCV{\_}2015{\_}paper}},
year = {2015}
}

@article{Finkel2005,
abstract = {Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9{\%} over state-of-the-art systems on two established information extraction tasks.},
author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher},
doi = {10.3115/1219840.1219885},
file = {:Users/rudolfkadlec/Downloads/FinkelGM05.pdf:pdf},
issn = {02773791},
journal = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics ACL 05},
number = {June},
pages = {363--370},
title = {{Incorporating non-local information into information extraction systems by Gibbs sampling}},
url = {http://portal.acm.org/citation.cfm?doid=1219840.1219885},
volume = {43},
year = {2005}
}


@article{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24{\%} accuracy on the Penn Treebank WSJ, an error reduction of 4.4{\%} on the best previous single automatically learned tagging result},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D},
doi = {10.3115/1073445.1073478},
file = {:Users/rudolfkadlec/Downloads/tagging.pdf:pdf},
journal = {In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL '03),},
pages = {252--259},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://dl.acm.org/citation.cfm?id=1073478},
year = {2003}
}


@article{Trischler2016,
abstract = {Understanding unstructured text is a major goal within natural language processing. Comprehension tests pose questions based on short text passages to evaluate such understanding. In this work, we investigate machine comprehension on the challenging {\{}$\backslash$it MCTest{\}} benchmark. Partly because of its limited size, prior work on {\{}$\backslash$it MCTest{\}} has focused mainly on engineering better features. We tackle the dataset with a neural approach, harnessing simple neural networks arranged in a parallel hierarchy. The parallel hierarchy enables our model to compare the passage, question, and answer from a variety of trainable perspectives, as opposed to using a manually designed, rigid feature set. Perspectives range from the word level to sentence fragments to sequences of sentences; the networks operate only on word-embedding representations of text. When trained with a methodology designed to help cope with limited training data, our Parallel-Hierarchical model sets a new state of the art for {\{}$\backslash$it MCTest{\}}, outperforming previous feature-engineered approaches slightly and previous neural approaches by a significant margin (over 15$\backslash${\%} absolute).},
archivePrefix = {arXiv},
arxivId = {1603.08884},
author = {Trischler, Adam and Ye, Zheng and Yuan, Xingdi and He, Jing and Bachman, Phillip and Suleman, Kaheer},
eprint = {1603.08884},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1603.08884v1.pdf:pdf},
journal = {Proceedings of ACL},
mendeley-groups = {DeepLearning},
title = {{A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data}},
url = {http://arxiv.org/abs/1603.08884},
year = {2016}
}


@article{Li2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1607.06275v1},
author = {Li, Peng and Li, Wei and He, Zhengyan and Wang, Xuguang and Cao, Ying and Zhou, Jie and Xu, Wei},
eprint = {arXiv:1607.06275v1},
file = {:Users/rudolfkadlec/Downloads/1607.06275.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering}},
year = {2016}
}


@article{Santos2016,
abstract = {In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.03609},
author = {dos Santos, Cicero and Tan, Ming and Xiang, Bing and Zhou, Bowen},
eprint = {1602.03609},
file = {:Users/rudolfkadlec/Library/Application Support/Mendeley Desktop/Downloaded/Santos et al. - 2016 - Attentive Pooling Networks.pdf:pdf},
mendeley-groups = {DeepLearning},
number = {Cv},
title = {{Attentive Pooling Networks}},
url = {http://arxiv.org/abs/1602.03609},
year = {2016}
}


@article{Mostafazadeh2016,
abstract = {Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of {\~{}}50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.},
archivePrefix = {arXiv},
arxivId = {1604.01696},
author = {Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Devi and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James},
eprint = {1604.01696},
file = {:Users/rudolfkadlec/Downloads/1604.01696v1.pdf:pdf},
journal = {Proceedings of NAACL},
title = {{A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories}},
url = {http://arxiv.org/abs/1604.01696},
year = {2016}
}

@article{Hewlett2016,
author = {Hewlett, Daniel and Lacoste, Alexandre and Jones, Llion and Polosukhin, Illia and Fandrianto, Andrew and Han, Jay and Kelcey, Matthew and Berthelot, David},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Desktop/Dropbox/Good{\_}Reader/Papers/Datasets/google16{\_}WikiReading.pdf:pdf},
journal = {Acl 2016},
pages = {1535--1545},
title = {{WIKI READING : A Novel Large-scale Language Understanding Task over Wikipedia}},
year = {2016}
}


@inproceedings{NELL-aaai15, Title = {Never-Ending Learning}, Author = {T. Mitchell and W. Cohen and E. Hruschka and P. Talukdar and J. Betteridge and A. Carlson and B. Dalvi and M. Gardner and B. Kisiel and J. Krishnamurthy and N. Lao and K. Mazaitis and T. Mohamed and N. Nakashole and E. Platanios and A. Ritter and M. Samadi and B. Settles and R. Wang and D. Wijaya and A. Gupta and X. Chen and A. Saparov and M. Greaves and J. Welling}, Booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)}, Year = {2015}} 

@article{Kawakami2015,
abstract = {We present a neural network architecture based on bidirectional LSTMs to compute representations of words in the sentential contexts. These context-sensitive word representations are suitable for, e.g., distinguishing different word senses and other context-modulated variations in meaning. To learn the parameters of our model, we use cross-lingual supervision, hypothesizing that a good representation of a word in context will be one that is sufficient for selecting the correct translation into a second language. We evaluate the quality of our representations as features in three downstream tasks: prediction of semantic supersenses (which assign nouns and verbs into a few dozen semantic classes), low resource machine translation, and a lexical substitution task, and obtain state-of-the-art results on all of these.},
archivePrefix = {arXiv},
arxivId = {1511.04623},
author = {Kawakami, Kazuya and Dyer, Chris},
eprint = {1511.04623},
file = {:Users/rudolfkadlec/Downloads/1511.04623v2.pdf:pdf},
journal = {ICLR Workshop Track},
mendeley-groups = {DeepLearning},
title = {{Learning to Represent Words in Context with Multilingual Supervision}},
url = {http://arxiv.org/abs/1511.04623},
year = {2016}
}


@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:Users/rudolfkadlec/Downloads/1301.3781.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}


@article{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:Users/rudolfkadlec/Library/Application Support/Mendeley Desktop/Downloaded/Pennington, Socher, Manning - 2014 - GloVe Global Vectors for Word Representation.pdf:pdf},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
mendeley-groups = {DeepLearning},
pages = {1532--1543},
title = {{GloVe: Global Vectors for Word Representation}},
year = {2014}
}

@article{Onishi2016,
abstract = {We have constructed a new "Who-did-What" dataset of over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus. The WDW dataset has a variety of novel features. First, in contrast with the CNN and Daily Mail datasets (Hermann et al., 2015) we avoid using article summaries for question formation. Instead, each problem is formed from two independent articles --- an article given as the passage to be read and a separate article on the same events used to form the question. Second, we avoid anonymization --- each choice is a person named entity. Third, the problems have been filtered to remove a fraction that are easily solved by simple baselines, while remaining 84{\%} solvable by humans. We report performance benchmarks of standard systems and propose the WDW dataset as a challenge task for the community.},
archivePrefix = {arXiv},
arxivId = {1608.05457},
author = {Onishi, Takeshi and Wang, Hai and Bansal, Mohit and Gimpel, Kevin and McAllester, David},
eprint = {1608.05457},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Desktop/Dropbox/Good{\_}Reader/Papers/QA/toyota16{\_}WhoDidWhat{\_}LargeScalePersonCenteredClozeDataset.pdf:pdf},
number = {3},
title = {{Who did What: A Large-Scale Person-Centered Cloze Dataset}},
url = {http://arxiv.org/abs/1608.05457},
year = {2016}
}

@article{Shen2016,
abstract = {Teaching a computer to read a document and answer general questions pertaining to the document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called Reasoning Network ({\{}ReasoNet{\}}) for machine comprehension tasks. ReasoNet makes use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNet introduces a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNet can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNet has achieved state-of-the-art performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, and a structured Graph Reachability dataset.},
archivePrefix = {arXiv},
arxivId = {1609.05284},
author = {Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu},
eprint = {1609.05284},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Desktop/Dropbox/Good{\_}Reader/Papers/QA/microsoft16{\_}ReasoNet{\_}LearningToStopReadingInMachineComprehension.pdf:pdf},
title = {{ReasoNet: Learning to Stop Reading in Machine Comprehension}},
url = {http://arxiv.org/abs/1609.05284},
year = {2016}
}


@article{Berg2016,
abstract = {In this work we study variance in the results of neural network training on a wide variety of configurations in automatic speech recognition. Although this variance itself is well known, this is, to the best of our knowledge, the first paper that performs an extensive empirical study on its effects in speech recognition. We view training as sampling from a distribution and show that these distributions can have a substantial variance. These results show the urgent need to rethink the way in which results in the literature are reported and interpreted.},
archivePrefix = {arXiv},
arxivId = {1606.04521},
author = {van den Berg, Ewout and Ramabhadran, Bhuvana and Picheny, Michael},
eprint = {1606.04521},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/Downloads/1606.04521.pdf:pdf},
mendeley-groups = {DeepLearning},
title = {{Training variance and performance evaluation of neural networks in speech}},
year = {2016},
journal = {arXiv}, 
volume = {1606.04521}
}

@article{Graves2016,
abstract = {Modern computers separate computation and memory. Computation is performed by a processor, which can use an addressable memory to bring operands in and out of play. This confers two important benefits: the use of extensible storage to write new information and the ability to treat the contents of memory as variables. Variables are critical to algorithm generality: to perform the same procedure on one datum or another, an algorithm merely has to change the address it reads from. In contrast to computers, the computational and memory resources of artificial neural networks are mixed together in the network weights and neuron activity. This is a major liability: as the memory demands of a task increase, these networks cannot allocate new storage dynam-ically, nor easily learn algorithms that act independently of the values realized by the task variables. Although recent breakthroughs demonstrate that neural networks are remarkably adept at sensory processing 1 , sequence learning 2,3 and reinforcement learning 4 , cognitive scientists and neuroscientists have argued that neural networks are limited in their ability to represent variables and data structures 5–9 , and to store data over long timescales without interference 10,11 . We aim to combine the advantages of neu-ral and computational processing by providing a neural network with read–write access to external memory. The access is narrowly focused, minimizing interference among memoranda and enabling long-term storage 12,13 . The whole system is differentiable, and can therefore be trained end-to-end with gradient descent, allowing the network to learn how to operate and organize the memory in a goal-directed manner.},
author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and {G{\'{o}}mez Colmenarejo}, Sergio and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and {Moritz Hermann}, Karl and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
doi = {10.1038/nature20101},
file = {:C$\backslash$:/Users/IBM{\_}ADMIN/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves et al. - 2016 - Hybrid computing using a neural network with dynamic external memory.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {DeepLearning},
pages = {471----476},
title = {{Hybrid computing using a neural network with dynamic external memory}},
volume = {538},
year = {2016}
}

@inproceedings{Koehn2004,
  title={Statistical Significance Tests for Machine Translation Evaluation.},
  author={Koehn, Philipp},
  booktitle={EMNLP},
  pages={388--395},
  year={2004},
  organization={Citeseer}
}

@inproceedings{bisani2004bootstrap,
  title={Bootstrap estimates for confidence intervals in ASR performance evaluation},
  author={Bisani, Maximilian and Ney, Hermann},
  booktitle={Acoustics, Speech, and Signal Processing, 2004. Proceedings.(ICASSP'04). IEEE International Conference on},
  volume={1},
  pages={I--409},
  year={2004},
  organization={IEEE}
}

@inproceedings{sogaard2014,
  title={What's in a p-value in NLP?},
  author={S{\o}gaard, Anders and Johannsen, Anders and Plank, Barbara and Hovy, Dirk and Alonso, H{\'e}ctor Mart{\'\i}nez},
  booktitle={CoNLL},
  pages={1--10},
  year={2014},
  organization={Citeseer}
}

@misc{drummond2009,
           month = {June},
           title = {Replicability is not Reproducibility: Nor is it Good Science},
          author = {Chris Drummond},
            year = {2009},
             url = {http://cogprints.org/7691/},
        abstract = {  At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our field. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientific discipline, being able to replicate experiments is paramount.  I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important differences between the two.  Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.}
}

@misc{peng2014,
author = {Peng, Roger},
title = {The Real Reason Reproducible Research is Important},
journal = {Simply Statistics},
type = {Blog},
number = {June 06},
year = {2014},
howpublished = {\url{https://simplystatistics.org/2014/06/06/the-real-reason-reproducible-research-is-important/}}
}


@book{popper1959logic,
  title={The logic of scientific discovery},
  author={Popper, Karl R},
  year={1959}
  %publisher={Basic Books}
}

@inproceedings{nadeau2000,
  title={Inference for the generalization error},
  author={Nadeau, Claude and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={307--313},
  year={2000}
}

@article{bengio2004,
  title={No unbiased estimator of the variance of k-fold cross-validation},
  author={Bengio, Yoshua and Grandvalet, Yves},
  journal={Journal of machine learning research},
  volume={5},
  number={Sep},
  pages={1089--1105},
  year={2004}
}

@article{demsar2006,
  title={Statistical comparisons of classifiers over multiple data sets},
  author={Dem{\v{s}}ar, Janez},
  journal={Journal of Machine learning research},
  volume={7},
  number={Jan},
  pages={1--30},
  year={2006}
}

@article{berrar2013,
author = { Daniel   Berrar  and  Jose   A.   Lozano },
title = {Significance tests or confidence intervals: which are preferable for the comparison of classifiers?},
journal = {Journal of Experimental \& Theoretical Artificial Intelligence},
volume = {25},
number = {2},
pages = {189-206},
year = {2013},
doi = {10.1080/0952813X.2012.680252},

URL = { 
        http://dx.doi.org/10.1080/0952813X.2012.680252
    
},
eprint = { 
        http://dx.doi.org/10.1080/0952813X.2012.680252
    
}
}

@article{santafe2015dealing,
  title={Dealing with the evaluation of supervised classification algorithms},
  author={Santafe, Guzman and Inza, I{\~n}aki and Lozano, Jose A},
  journal={The Artificial Intelligence Review},
  volume={44},
  number={4},
  pages={467},
  year={2015},
  publisher={Springer Science \& Business Media}
}

@article{berrar2013significance,
  title={Significance tests or confidence intervals: which are preferable for the comparison of classifiers?},
  author={Berrar, Daniel and Lozano, Jose A},
  journal={Journal of Experimental \& Theoretical Artificial Intelligence},
  volume={25},
  number={2},
  pages={189--206},
  year={2013},
  publisher={Taylor \& Francis}
}

@article {Gardner1989,
	author = {Gardner, M J and Altman, D G},
	title = {Confidence intervals rather than P values: estimation rather than hypothesis testing.},
	volume = {292},
	number = {6522},
	pages = {746--750},
	year = {1986},
	publisher = {BMJ Publishing Group Ltd},
	abstract = {Overemphasis on hypothesis testing--and the use of P values to dichotomise significant or non-significant results--has detracted from more useful approaches to interpreting study results, such as estimation and confidence intervals. In medical studies investigators are usually interested in determining the size of difference of a measured outcome between groups, rather than a simple indication of whether or not it is statistically significant. Confidence intervals present a range of values, on the basis of the sample data, in which the population value for such a difference may lie. Some methods of calculating confidence intervals for means and differences between means are given, with similar information for proportions. The paper also gives suggestions for graphical display. Confidence intervals, if appropriate to the type of study, should be used for major findings in both the main text of a paper and its abstract.},
	journal = {British Medical Journal}
}

@inproceedings{huang2016densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q and van der Maaten, Laurens},
  booktitle={CVPR},
  year={2017}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  year={2016}
}

@article{kadlec2017knowledge,
  title={Knowledge Base Completion: Baselines Strike Back},
  author={Kadlec, Rudolf and Bajgar, Ondrej and Kleindienst, Jan},
  journal={arXiv preprint arXiv:1705.10744},
  year={2017}
}

 
 
 @article{Yang2015,
abstract = {We consider learning representations of entities and relations in KBs using the neural-embedding approach. We show that most existing models, including NTN (Socher et al., 2013) and TransE (Bordes et al., 2013b), can be generalized under a unified learning framework, where entities are low-dimensional vectors learned from a neural network and relations are bilinear and/or linear mapping functions. Under this framework, we compare a variety of embedding models on the link prediction task. We show that a simple bilinear formulation achieves new state-of-the-art results for the task (achieving a top-10 accuracy of 73.2{\%} vs. 54.7{\%} by TransE on Freebase). Furthermore, we introduce a novel approach that utilizes the learned relation embeddings to mine logical rules such as "BornInCity(a,b) and CityInCountry(b,c) ={\textgreater} Nationality(a,c)". We find that embeddings learned from the bilinear objective are particularly good at capturing relational semantics and that the composition of relations is characterized by matrix multiplication. More interestingly, we demonstrate that our embedding-based rule extraction approach successfully outperforms a state-of-the-art confidence-based rule mining approach in mining Horn rules that involve compositional reasoning.},
archivePrefix = {arXiv},
arxivId = {1412.6575},
author = {Yang, Bishan and Yih, Wen-tau and He, Xiaodong and Gao, Jianfeng and Deng, Li},
eprint = {1412.6575},
file = {:Users/rudolfkadlec/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2015 - Embedding Entities and Relations for Learning and Inference in Knowledge Bases.pdf:pdf},
journal = {ICLR},
mendeley-groups = {DeepLearning},
pages = {12},
pmid = {633505},
title = {{Embedding Entities and Relations for Learning and Inference in Knowledge Bases}},
url = {http://arxiv.org/abs/1412.6575},
year = {2015}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  year={2009},
  publisher={Technical report, University of Toronto}
}

@incollection{Bordes2013,
title = {Translating Embeddings for Modeling Multi-relational Data},
author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Duran, Alberto and Weston, Jason and Yakhnenko, Oksana},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {2787--2795},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf}
}

@inproceedings{munkhdalai2016reasoning,
  title={Reasoning with memory augmented neural networks for language comprehension},
  author={Munkhdalai, Tsendsuren and Yu, Hong},
  booktitle={ICLR},
  year={2017}
}

@article{efron1987better,
 abstract = {We consider the problem of setting approximate confidence intervals for a single parameter θ in a multiparameter family. The standard approximate intervals based on maximum likelihood theory, θ̂ ± σ̂z<sup>(α)</sup>, can be quite misleading. In practice, tricks based on transformations, bias corrections, and so forth, are often used to improve their accuracy. The bootstrap confidence intervals discussed in this article automatically incorporate such tricks without requiring the statistician to think them through for each new application, at the price of a considerable increase in computational effort. The new intervals incorporate an improvement over previously suggested methods, which results in second-order correctness in a wide variety of problems. In addition to parametric families, bootstrap intervals are also developed for nonparametric situations.},
 author = {Bradley Efron},
 journal = {Journal of the American Statistical Association},
 number = {397},
 pages = {171-185},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Better Bootstrap Confidence Intervals},
 volume = {82},
 year = {1987}
}

@article{anderson1954atest,
author = { T. W.   Anderson  and  D. A.   Darling },
title = {A Test of Goodness of Fit},
journal = {Journal of the American Statistical Association},
volume = {49},
number = {268},
pages = {765-769},
year = {1954}
}

@article{jensen2000multiple,
  title={Multiple comparisons in induction algorithms},
  author={Jensen, David D and Cohen, Paul R},
  journal={Machine Learning},
  volume={38},
  number={3},
  pages={309--338},
  year={2000},
  publisher={Springer}
}

@inproceedings{fokkens2013offspring,
  title={Offspring from Reproduction Problems: What Replication Failure Teaches Us.},
  author={Fokkens, Antske and Van Erp, Marieke and Postma, Marten and Pedersen, Ted and Vossen, Piek and Freire, Nuno},
  booktitle={ACL},
  year={2013}
}

@inproceedings{raeder2010consequences,
  title={Consequences of variability in classifier performance estimates},
  author={Raeder, Troy and Hoens, T Ryan and Chawla, Nitesh V},
  booktitle={IEEE 10th International Conference on Data Mining (ICDM)},
  pages={421--430},
  year={2010}}





@inproceedings{henderson2017deep,
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
title = {Deep Reinforcement Learning that Matters},
booktitle = {AAAI},
year = {2018},
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.}
}

@inproceedings{wagstaff2012machine,
  title={Machine Learning that Matters},
  author={Wagstaff, Kiri L},
  booktitle={Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  pages={529--536},
  year={2012}
}

@article{boulesteix2013plea,
  title={A plea for neutral comparison studies in computational sciences},
  author={Boulesteix, Anne-Laure and Lauer, Sabine and Eugster, Manuel JA},
  journal={PloS one},
  volume={8},
  number={4},
  pages={e61562},
  year={2013},
  publisher={Public Library of Science}
}

@article{ioannidis2005most,
  title={Why most published research findings are false},
  author={Ioannidis, John PA},
  journal={PLoS medicine},
  volume={2},
  number={8},
  pages={e124},
  year={2005},
  publisher={Public Library of Science}
}

@article{ince2012case,
  title={The case for open computer programs},
  author={Ince, Darrel C and Hatton, Leslie and Graham-Cumming, John},
  journal={Nature},
  volume={482},
  number={7386},
  pages={485},
  year={2012},
  publisher={Nature Publishing Group}
}

@inproceedings{ruml2010logic,
  title={The logic of benchmarking: A case against state-of-the-art performance},
  author={Ruml, Wheeler},
  booktitle={Third Annual Symposium on Combinatorial Search},
  year={2010}
}

@book{cohen1995empirical,
  title={Empirical methods for artificial intelligence},
  author={Cohen, Paul R},
  year={1995},
  publisher={MIT press Cambridge, MA}
}

@inproceedings{reimers2017reporting,
  title={Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging},
  author={Reimers, Nils and Gurevych, Iryna},
  booktitle={EMNLP},
  year={2017}
}

@article{scott1992multivariate,
  title={Multivariate Density Estimation: Theory, Practice, and Visualization},
  author={Scott, David W},
  publisher={Wiley},
  year={1992}
}

@inproceedings{yang2016words,
  title={Words or characters? fine-grained gating for reading comprehension},
  author={Yang, Zhilin and Dhingra, Bhuwan and Yuan, Ye and Hu, Junjie and Cohen, William W and Salakhutdinov, Ruslan},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{wang2016machine,
  title={Machine comprehension using match-lstm and answer pointer},
  author={Wang, Shuohang and Jiang, Jing},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{seo2016bidirectional,
  title={Bidirectional attention flow for machine comprehension},
  author={Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{xiong2016dynamic,
  title={Dynamic coattention networks for question answering},
  author={Xiong, Caiming and Zhong, Victor and Socher, Richard},
  booktitle={ICLR},
  year={2017}
}

@book{arnold2008first,
  title={A First Course in Order Statistics},
  author={Arnold, Barry C and Balakrishnan, N and Nagaraja, H N},
  year={2008},
  publisher={SIAM}
}


@inproceedings{sculley2018winner,
  title={Winner's Curse? On Pace, Progress, and Empirical Rigor},
  author={Sculley, D and Snoek, Jasper and Wiltschko, Alex and Rahimi, Ali},
  year={2018},
  booktitle={ICLR Workshop}
}
